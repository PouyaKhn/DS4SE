{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rEykIUCC9eHr",
   "metadata": {
    "id": "rEykIUCC9eHr"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-feature",
   "metadata": {
    "id": "dressed-feature"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "duplicate_dataset = pd.read_csv('./drive/MyDrive/DuplicateQuestions.csv')\n",
    "duplicate_dataset = duplicate_dataset.sample(frac=0.2)\n",
    "\n",
    "posts = duplicate_dataset.OBody.tolist()\n",
    "titles = duplicate_dataset.OTitle.tolist()\n",
    "tags = duplicate_dataset.OTags.tolist()\n",
    "\n",
    "duplicated_posts = duplicate_dataset.DBody.tolist()\n",
    "duplicated_titles = duplicate_dataset.DTitle.tolist()\n",
    "duplicated_tags = duplicate_dataset.DTags.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-garbage",
   "metadata": {
    "id": "several-garbage"
   },
   "outputs": [],
   "source": [
    "not_duplicated_posts = []\n",
    "not_duplicated_titles = []\n",
    "not_duplicated_tags = []\n",
    "\n",
    "not_duplicate_dataset = pd.read_csv('./drive/MyDrive/NotDuplicateQuestions.csv')\n",
    "not_duplicate_dataset = not_duplicate_dataset.sample(frac=0.2)\n",
    "\n",
    "not_duplicated_posts = not_duplicate_dataset.Body.tolist()\n",
    "not_duplicated_titles = not_duplicate_dataset.Title.tolist()\n",
    "not_duplicated_tags = not_duplicate_dataset.Tags.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-welding",
   "metadata": {
    "id": "western-welding"
   },
   "source": [
    "# pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf85ffe9-e2c2-49e2-b70f-3fb24913ef6b",
   "metadata": {
    "id": "bf85ffe9-e2c2-49e2-b70f-3fb24913ef6b"
   },
   "source": [
    "- removing html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-earth",
   "metadata": {
    "id": "outdoor-earth"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html_tags(inputs):\n",
    "    for i in range(len(inputs)):\n",
    "        inputs[i] = BeautifulSoup(inputs[i], \"lxml\").text\n",
    "        \n",
    "# ------------------ Original questions from first dataset ---------------------\n",
    "remove_html_tags(posts)\n",
    "remove_html_tags(titles)\n",
    "remove_html_tags(tags)\n",
    "\n",
    "# ------------------ Duplicated questions from first dataset ------------------\n",
    "remove_html_tags(duplicated_posts)\n",
    "remove_html_tags(duplicated_titles)\n",
    "remove_html_tags(duplicated_tags)\n",
    "\n",
    "# ------------------ Not Duplicated questions from second dataset ------------------\n",
    "remove_html_tags(not_duplicated_posts)\n",
    "remove_html_tags(not_duplicated_titles)\n",
    "remove_html_tags(not_duplicated_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041dc935-26e0-47db-804e-27e8b0729b2b",
   "metadata": {
    "id": "041dc935-26e0-47db-804e-27e8b0729b2b"
   },
   "source": [
    "- lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-punch",
   "metadata": {
    "id": "naval-punch"
   },
   "outputs": [],
   "source": [
    "def lower_case(inputs):\n",
    "    for i in range(len(inputs)):\n",
    "        inputs[i] = inputs[i].lower()\n",
    "        \n",
    "# ------------------ Original questions from first dataset ---------------------\n",
    "lower_case(posts)\n",
    "lower_case(titles)\n",
    "lower_case(tags)\n",
    "\n",
    "# ------------------ Duplicated questions from first dataset ------------------\n",
    "lower_case(duplicated_posts)\n",
    "lower_case(duplicated_titles)\n",
    "lower_case(duplicated_tags)\n",
    "\n",
    "# ------------------ Not Duplicated questions from second dataset ------------------\n",
    "lower_case(not_duplicated_posts)\n",
    "lower_case(not_duplicated_titles)\n",
    "lower_case(not_duplicated_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ec3a6f-ed96-4b4d-a967-6f08fb79d7cc",
   "metadata": {
    "id": "67ec3a6f-ed96-4b4d-a967-6f08fb79d7cc"
   },
   "source": [
    "- removing numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-eating",
   "metadata": {
    "id": "inappropriate-eating"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_numbers(inputs):\n",
    "    for i in range(len(inputs)):\n",
    "        inputs[i] = re.sub(r'\\d+', '', inputs[i])\n",
    "\n",
    "# ------------------ Original questions from first dataset ---------------------\n",
    "remove_numbers(posts)\n",
    "remove_numbers(titles)\n",
    "remove_numbers(tags)\n",
    "\n",
    "# ------------------ Duplicated questions from first dataset ------------------\n",
    "remove_numbers(duplicated_posts)\n",
    "remove_numbers(duplicated_titles)\n",
    "remove_numbers(duplicated_tags)\n",
    "\n",
    "# ------------------ Not Duplicated questions from second dataset ------------------\n",
    "remove_numbers(not_duplicated_posts)\n",
    "remove_numbers(not_duplicated_titles)\n",
    "remove_numbers(not_duplicated_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b737ea9d-2855-4f6a-a377-7d4965ad16d9",
   "metadata": {
    "id": "b737ea9d-2855-4f6a-a377-7d4965ad16d9"
   },
   "source": [
    "- removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbce543-2a4c-4a27-b8ce-a2fba3f820b1",
   "metadata": {
    "id": "5dbce543-2a4c-4a27-b8ce-a2fba3f820b1"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_punct(inputs):\n",
    "    for i in range(len(inputs)):\n",
    "        inputs[i] = re.sub(r'[^\\w\\s]', '', inputs[i])\n",
    "\n",
    "# ------------------ Original questions from first dataset ---------------------\n",
    "remove_punct(posts)\n",
    "remove_punct(titles)\n",
    "remove_punct(tags)\n",
    "\n",
    "# ------------------ Duplicated questions from first dataset ------------------\n",
    "remove_punct(duplicated_posts)\n",
    "remove_punct(duplicated_titles)\n",
    "remove_punct(duplicated_tags)\n",
    "\n",
    "# ------------------ Not Duplicated questions from second dataset ------------------\n",
    "remove_punct(not_duplicated_posts)\n",
    "remove_punct(not_duplicated_titles)\n",
    "remove_punct(not_duplicated_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cc1a9c-0323-47be-b6aa-9eca84faea8c",
   "metadata": {
    "id": "a8cc1a9c-0323-47be-b6aa-9eca84faea8c"
   },
   "source": [
    "- Tokenization & removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-stage",
   "metadata": {
    "id": "respected-stage",
    "outputId": "4776ec81-a040-40c6-d817-1d8a4e9ba6be"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stop_words(inputs):\n",
    "    tokenized_list = []\n",
    "    for i in range(len(inputs)):\n",
    "        tokens = word_tokenize(inputs[i])\n",
    "        tokenized_list.append([j for j in tokens if j not in stop_words])\n",
    "    return tokenized_list\n",
    "\n",
    "# ------------------ Original questions from first dataset ---------------------\n",
    "tokenized_posts = remove_stop_words(posts)\n",
    "tokenized_titles = remove_stop_words(titles)\n",
    "tokenized_tags = remove_stop_words(tags)\n",
    "\n",
    "# ------------------ Duplicated questions from first dataset ------------------\n",
    "tokenized_duplicate_posts = remove_stop_words(duplicated_posts)\n",
    "tokenized_duplicate_titles = remove_stop_words(duplicated_titles)\n",
    "tokenized_duplicate_tags = remove_stop_words(duplicated_tags)\n",
    "\n",
    "# ------------------ Not Duplicated questions from second dataset ------------------\n",
    "tokenized_n_d_posts = remove_stop_words(not_duplicated_posts)\n",
    "tokenized_n_d_titles = remove_stop_words(not_duplicated_titles)\n",
    "tokenized_n_d_tags = remove_stop_words(not_duplicated_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35ae1d2-00f3-4e85-844d-d5c44c83b94c",
   "metadata": {
    "id": "b35ae1d2-00f3-4e85-844d-d5c44c83b94c"
   },
   "source": [
    "- lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-rental",
   "metadata": {
    "id": "royal-rental",
    "outputId": "004a0e70-27aa-4009-cefc-f5a123ced06c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatization(tokenized_input):\n",
    "    for i in range(len(tokenized_input)):\n",
    "        result = []\n",
    "        for word in tokenized_input[i]:\n",
    "            lemmatizer.lemmatize(word)\n",
    "            result.append(lemmatizer.lemmatize(word))\n",
    "        tokenized_input[i] = result\n",
    "    \n",
    "# ------------------ Original questions from first dataset ---------------------\n",
    "lemmatization(tokenized_posts)\n",
    "lemmatization(tokenized_titles)\n",
    "lemmatization(tokenized_tags)\n",
    "\n",
    "# ------------------ Duplicated questions from first dataset ------------------\n",
    "lemmatization(tokenized_duplicate_posts)\n",
    "lemmatization(tokenized_duplicate_titles)\n",
    "lemmatization(tokenized_duplicate_tags)\n",
    "\n",
    "# ------------------ Not Duplicated questions from second dataset ------------------\n",
    "lemmatization(tokenized_n_d_posts)\n",
    "lemmatization(tokenized_n_d_titles)\n",
    "lemmatization(tokenized_n_d_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d28f461-94f2-4e64-b1c2-c3a4571f71f8",
   "metadata": {
    "id": "2d28f461-94f2-4e64-b1c2-c3a4571f71f8"
   },
   "source": [
    "- Concatanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-technician",
   "metadata": {
    "id": "criminal-technician"
   },
   "outputs": [],
   "source": [
    "def concat(tokenized_input):\n",
    "    result = []\n",
    "    for t in tokenized_input:\n",
    "        result.append(' '.join(t))\n",
    "    return result\n",
    "\n",
    "posts = concat(tokenized_posts)\n",
    "titles = concat(tokenized_titles)\n",
    "tags = concat(tokenized_tags)\n",
    "duplicatePosts = concat(tokenized_duplicate_posts)\n",
    "duplicateTitles = concat(tokenized_duplicate_titles)\n",
    "duplicateTags = concat(tokenized_duplicate_tags)\n",
    "\n",
    "#\n",
    "posts__ = concat(tokenized_n_d_posts)\n",
    "titles__ = concat(tokenized_n_d_titles)\n",
    "tags__ = concat(tokenized_n_d_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-twins",
   "metadata": {
    "id": "distributed-twins"
   },
   "outputs": [],
   "source": [
    "# allPost = posts + duplicated_posts + posts__\n",
    "allTokenizedPosts = tokenized_posts + tokenized_duplicate_posts + tokenized_n_d_posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-turtle",
   "metadata": {
    "id": "comparative-turtle"
   },
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-franklin",
   "metadata": {
    "id": "governing-franklin",
    "outputId": "2cdedc63-58b1-4f31-bcfc-344eb3d7af7e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "documents = [TaggedDocument(d, [i]) for i, d in enumerate(allTokenizedPosts)]\n",
    "doc2vec_model = Doc2Vec(documents, vector_size=20, window=2, min_count=1, workers=4, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-evidence",
   "metadata": {
    "id": "convinced-evidence"
   },
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "duplicated_posts_features = []\n",
    "for i in range(len(posts)):\n",
    "    result = []\n",
    "\n",
    "    vector1 = doc2vec_model.infer_vector(posts[i].split())\n",
    "    vector2 = doc2vec_model.infer_vector(duplicatePosts[i].split())\n",
    "    cosine_distance = spatial.distance.cosine(vector1, vector2)\n",
    "    result.append(cosine_distance)\n",
    "    \n",
    "    vector1 = doc2vec_model.infer_vector(titles[i].split())\n",
    "    vector2 = doc2vec_model.infer_vector(duplicateTitles[i].split())\n",
    "    cosine_distance = spatial.distance.cosine(vector1, vector2)\n",
    "    result.append(cosine_distance)\n",
    "    \n",
    "    vector1 = doc2vec_model.infer_vector(tags[i].split())\n",
    "    vector2 = doc2vec_model.infer_vector(duplicateTags[i].split())\n",
    "    cosine_distance = spatial.distance.cosine(vector1, vector2)\n",
    "    result.append(cosine_distance)\n",
    "    \n",
    "    vector1 = doc2vec_model.infer_vector(posts[i].split())\n",
    "    vector2 = doc2vec_model.infer_vector(duplicateTitles[i].split())\n",
    "    cosine_distance = spatial.distance.cosine(vector1, vector2)\n",
    "    result.append(cosine_distance)\n",
    "    \n",
    "    vector1 = doc2vec_model.infer_vector(titles[i].split())\n",
    "    vector2 = doc2vec_model.infer_vector(duplicatePosts[i].split())\n",
    "    cosine_distance = spatial.distance.cosine(vector1, vector2)\n",
    "    result.append(cosine_distance)\n",
    "\n",
    "    duplicated_posts_features.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-partnership",
   "metadata": {
    "id": "quiet-partnership"
   },
   "outputs": [],
   "source": [
    "not_duplicated_posts_features = []\n",
    "for i in range(0, len(posts__), 2):\n",
    "    result = []\n",
    "\n",
    "    vector1 = doc2vec_model.infer_vector(posts__[i].split())\n",
    "    vector2 = doc2vec_model.infer_vector(posts__[i+1].split())\n",
    "    cosine_distance = spatial.distance.cosine(vector1, vector2)\n",
    "    result.append(cosine_distance)\n",
    "    \n",
    "    vector1 = doc2vec_model.infer_vector(titles__[i].split())\n",
    "    vector2 = doc2vec_model.infer_vector(titles__[i+1].split())\n",
    "    cosine_distance = spatial.distance.cosine(vector1, vector2)\n",
    "    result.append(cosine_distance)\n",
    "\n",
    "    vector1 = doc2vec_model.infer_vector(tags__[i].split())\n",
    "    vector2 = doc2vec_model.infer_vector(tags__[i+1].split())\n",
    "    cosine_distance = spatial.distance.cosine(vector1, vector2)\n",
    "    result.append(cosine_distance)\n",
    "    \n",
    "    vector1 = doc2vec_model.infer_vector(posts__[i].split())\n",
    "    vector2 = doc2vec_model.infer_vector(titles__[i+1].split())\n",
    "    cosine_distance = spatial.distance.cosine(vector1, vector2)\n",
    "    result.append(cosine_distance)\n",
    "\n",
    "    vector1 = doc2vec_model.infer_vector(titles__[i].split())\n",
    "    vector2 = doc2vec_model.infer_vector(posts__[i+1].split())\n",
    "    cosine_distance = spatial.distance.cosine(vector1, vector2)\n",
    "    result.append(cosine_distance)\n",
    "\n",
    "    not_duplicated_posts_features.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-marriage",
   "metadata": {
    "id": "premium-marriage"
   },
   "source": [
    "# lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-accreditation",
   "metadata": {
    "id": "efficient-accreditation"
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "dictionary = Dictionary(allTokenizedPosts)\n",
    "\n",
    "corpus = [dictionary.doc2bow(post) for post in allTokenizedPosts]\n",
    "\n",
    "lda_model =  LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-actor",
   "metadata": {
    "id": "cognitive-actor"
   },
   "outputs": [],
   "source": [
    "from gensim.matutils import cossim\n",
    "\n",
    "corpus_p = [dictionary.doc2bow(post) for post in tokenized_posts]\n",
    "corpus_dup_p = [dictionary.doc2bow(post) for post in tokenized_duplicate_posts]\n",
    "\n",
    "counter = 0\n",
    "for i in range(len(corpus_p)):\n",
    "    vector1 = lda_model.get_document_topics(corpus_p[i], minimum_probability=0)\n",
    "    vector2 = lda_model.get_document_topics(corpus_dup_p[i], minimum_probability=0)\n",
    "    coss_similarity = cossim(vector1, vector2)\n",
    "    duplicated_posts_features[counter].append(coss_similarity)\n",
    "    \n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-guitar",
   "metadata": {
    "id": "valid-guitar"
   },
   "outputs": [],
   "source": [
    "from gensim.matutils import cossim\n",
    "\n",
    "corpusP__ = [dictionary.doc2bow(post) for post in tokenized_n_d_posts]\n",
    "\n",
    "counter = 0\n",
    "for i in range(0, len(posts__), 2):\n",
    "    vector1 = lda_model.get_document_topics(corpusP__[i], minimum_probability=0)\n",
    "    vector2 = lda_model.get_document_topics(corpusP__[i+1], minimum_probability=0)\n",
    "    coss_similarity = cossim(vector1, vector2)\n",
    "    not_duplicated_posts_features[counter].append(coss_similarity)\n",
    "    \n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-comparison",
   "metadata": {
    "id": "affiliated-comparison"
   },
   "source": [
    "# Relevance similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-gospel",
   "metadata": {
    "id": "multiple-gospel"
   },
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "bm25 = BM25Okapi(allTokenizedPosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-proposal",
   "metadata": {
    "id": "perceived-proposal"
   },
   "outputs": [],
   "source": [
    "for i in range(len(posts)):\n",
    "    scores = bm25.get_scores(tokenized_posts[i])\n",
    "    duplicated_posts_features[i].append(scores[2*i+1])\n",
    "\n",
    "    scores = bm25.get_scores(tokenized_titles[i])\n",
    "    duplicated_posts_features[i].append(scores[2*i+1])\n",
    "\n",
    "    scores = bm25.get_scores(tokenized_tags[i])\n",
    "    duplicated_posts_features[i].append(scores[2*i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-transaction",
   "metadata": {
    "id": "committed-transaction"
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for i in range(0, len(posts__), 2):\n",
    "    scores = bm25.get_scores(tokenized_n_d_posts[i])\n",
    "    not_duplicated_posts_features[counter].append(scores[2*i+1])\n",
    "    \n",
    "    scores = bm25.get_scores(tokenized_n_d_titles[i])\n",
    "    not_duplicated_posts_features[counter].append(scores[2*i+1])\n",
    "\n",
    "    scores = bm25.get_scores(tokenized_n_d_tags[i])\n",
    "    not_duplicated_posts_features[counter].append(scores[2*i+1])\n",
    "\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffd44d4-8a4d-437c-84d1-c665d0ea6cd5",
   "metadata": {
    "id": "fffd44d4-8a4d-437c-84d1-c665d0ea6cd5"
   },
   "source": [
    "# Add labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-translation",
   "metadata": {
    "id": "failing-translation"
   },
   "outputs": [],
   "source": [
    "for i in duplicated_posts_features:\n",
    "    i.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-group",
   "metadata": {
    "id": "paperback-group"
   },
   "outputs": [],
   "source": [
    "for i in not_duplicated_posts_features:\n",
    "    i.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd1095a-270e-4495-9ecf-6a47b49213fc",
   "metadata": {
    "id": "8cd1095a-270e-4495-9ecf-6a47b49213fc"
   },
   "source": [
    "# Write Output with label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-terry",
   "metadata": {
    "id": "integrated-terry"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "filename = \"./drive/MyDrive/duplicated_scores.csv\"\n",
    "\n",
    "with open(filename, 'w') as csvfile:  \n",
    "    csvwriter = csv.writer(csvfile)  \n",
    "        \n",
    "    csvwriter.writerows([[1,2,3,4,5,6,7,8,9,10]] + duplicated_posts_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-campbell",
   "metadata": {
    "id": "crude-campbell"
   },
   "outputs": [],
   "source": [
    "filename = \"./drive/MyDrive/not_duplicated_scores.csv\"\n",
    "\n",
    "with open(filename, 'w') as csvfile:  \n",
    "    csvwriter = csv.writer(csvfile)  \n",
    "        \n",
    "    csvwriter.writerows([[1,2,3,4,5,6,7,8,9,10]] + not_duplicated_posts_features)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "answer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
